{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\chris\\\\source\\\\repos\\\\behaviour_recognition\\\\behaviour_recognition\\\\utils.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import importlib\n",
    "import math\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "FONTS =cv.FONT_HERSHEY_COMPLEX\n",
    "closed_eyes_frame = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_counter = 0\n",
    "closed_eyes_counter = 0\n",
    "total_blinks = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark detection function \n",
    "def landmarksDetection(img, results, draw=False):\n",
    "    img_height, img_width= img.shape[:2]\n",
    "    # list[(x,y), (x,y)....]\n",
    "    mesh_coord = [(int(point.x * img_width), int(point.y * img_height)) for point in results.multi_face_landmarks[0].landmark]\n",
    "    if draw :\n",
    "        [cv.circle(img, p, 2, utils.GREEN, -1) for p in mesh_coord]\n",
    "\n",
    "    # returning the list of tuples for each landmarks \n",
    "    return mesh_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face bounder indices \n",
    "FACE_OVAL=[ 10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103,67, 109]\n",
    "\n",
    "# lips indices for Landmarks\n",
    "LIPS=[ 61, 146, 91, 181, 84, 17, 314, 405, 321, 375,291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95,185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78 ]\n",
    "LOWER_LIPS =[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
    "UPPER_LIPS=[ 185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78] \n",
    "# Left eyes indices \n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
    "\n",
    "# right eyes indices\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]  \n",
    "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blink calculator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclaideanDistance(point, point1):\n",
    "    x, y = point \n",
    "    x1, y1 = point1\n",
    "    distance = math.sqrt((x1-x)**2 + (y1-y)**2)\n",
    "    return distance \n",
    "\n",
    "def blinkRatio(img, landmarks, right_indices, left_indices):\n",
    "    \n",
    "    # RIGHT_EYE\n",
    "    # horizontal line\n",
    "    rh_right = landmarks[right_indices[0]]\n",
    "    rh_left = landmarks[right_indices[8]]\n",
    "    # vertical line\n",
    "    rv_top = landmarks[right_indices[12]]\n",
    "    rv_bottom = landmarks[right_indices[4]]\n",
    "\n",
    "    # LEFT_EYE\n",
    "    # horizontal line\n",
    "    lh_right = landmarks[right_indices[0]]\n",
    "    lh_left = landmarks[right_indices[8]]\n",
    "    # vertical line\n",
    "    lv_top = landmarks[right_indices[12]]\n",
    "    lv_bottom = landmarks[right_indices[4]]\n",
    "\n",
    "    rh_distance = euclaideanDistance(rh_right, rh_left)\n",
    "    rv_distance = euclaideanDistance(rv_top, rv_bottom)\n",
    "    lv_distance = euclaideanDistance(lv_top, lv_bottom)\n",
    "    lh_distance = euclaideanDistance(lh_right, lh_left)\n",
    "\n",
    "    right_eye_ratio = rh_distance / rv_distance\n",
    "    left_eye_ratio = lh_distance / lv_distance\n",
    "\n",
    "    ratio = (right_eye_ratio + left_eye_ratio) / 2\n",
    "    \n",
    "    cv.line(frame, rh_right, rh_left, utils.GREEN, 2)\n",
    "    cv.line(frame, rv_top, rv_bottom, utils.GREEN, 2)\n",
    "    return ratio\n",
    "\n",
    "def yawnRatio(img, landmarks, lips_indices):\n",
    "    \n",
    "    # mouth\n",
    "    # horizontal line\n",
    "    lip_right = landmarks[lips_indices[0]]\n",
    "    lip_left = landmarks[lips_indices[10]]\n",
    "    # vertical line\n",
    "    lip_top = landmarks[lips_indices[16]]\n",
    "    lip_bottom = landmarks[lips_indices[34]]\n",
    "\n",
    "    lip_horizontal_distance = euclaideanDistance(lip_right, lip_left)\n",
    "    lip_vertical_distance = euclaideanDistance(lip_top, lip_bottom)\n",
    "\n",
    "\n",
    "    lip_ratio = lip_horizontal_distance / lip_vertical_distance\n",
    "    \n",
    "    #ratio = (right_eye_ratio + left_eye_ratio) / 2\n",
    "    \n",
    "    cv.line(frame, lip_right, lip_left, utils.GREEN, 2)\n",
    "    cv.line(frame, lip_top, lip_bottom, utils.GREEN, 2)\n",
    "    #eturn ratio    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\source\\repos\\behaviour_recognition\\behaviour_recognition\\face_phase_detector.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=38'>39</a>\u001b[0m cv\u001b[39m.\u001b[39mputText(frame, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal blinks \u001b[39m\u001b[39m{\u001b[39;00mtotal_blinks\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, (\u001b[39m100\u001b[39m,\u001b[39m150\u001b[39m), FONTS, \u001b[39m0.6\u001b[39m, utils\u001b[39m.\u001b[39mGREEN, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=41'>42</a>\u001b[0m \u001b[39m## Yawn\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=42'>43</a>\u001b[0m ratio_mouth \u001b[39m=\u001b[39m yawnRatio(frame, mesh_coords, LIPS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=43'>44</a>\u001b[0m \u001b[39m#cv.putText(frame, f'ratio {round(ratio_mouth,2)}', (100,100), FONTS, 1.0, utils.GREEN, 1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=44'>45</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=45'>46</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=46'>47</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=47'>48</a>\u001b[0m \u001b[39m## calculating  frame per seconds FPS\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=48'>49</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_time\n",
      "\u001b[1;32mc:\\Users\\chris\\source\\repos\\behaviour_recognition\\behaviour_recognition\\face_phase_detector.ipynb Cell 11\u001b[0m in \u001b[0;36myawnRatio\u001b[1;34m(img, landmarks, lips_indices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=48'>49</a>\u001b[0m lip_horizontal_distance \u001b[39m=\u001b[39m euclaideanDistance(lip_right, lip_left)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=49'>50</a>\u001b[0m lip_vertical_distance \u001b[39m=\u001b[39m euclaideanDistance(lip_top, lip_bottom)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=52'>53</a>\u001b[0m lip_ratio \u001b[39m=\u001b[39m lip_horizontal_distance \u001b[39m/\u001b[39;49m lip_vertical_distance\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=54'>55</a>\u001b[0m \u001b[39m#ratio = (right_eye_ratio + left_eye_ratio) / 2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/source/repos/behaviour_recognition/behaviour_recognition/face_phase_detector.ipynb#ch0000011?line=56'>57</a>\u001b[0m cv\u001b[39m.\u001b[39mline(frame, lip_right, lip_left, utils\u001b[39m.\u001b[39mGREEN, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Connect to cam\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1, \n",
    "    refine_landmarks=True, \n",
    "    min_detection_confidence=0.5, \n",
    "    min_tracking_confidence=0.5\n",
    ") as face_mesh:\n",
    "\n",
    "    # starting time here \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_counter +=1 # frame counter\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Show image\n",
    "        frame = cv.flip(frame, 1)\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        img_height, img_width = frame.shape[:2]   # This is the actual measurements of the frame size.  We'll use this to multiply by the normalised x,y coordinates from results.multi_face_landmarks\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        if results.multi_face_landmarks:\n",
    "            mesh_coords = landmarksDetection(frame, results, False)\n",
    "            #print(mesh_coords[p] for p in RIGHT_EYE)\n",
    "            \n",
    "            ## Blink\n",
    "            ratio_eyes = blinkRatio(frame, mesh_coords, RIGHT_EYE, LEFT_EYE)\n",
    "            cv.putText(frame, f'ratio {round(ratio_eyes,2)}', (100,100), FONTS, 1.0, utils.GREEN, 1)\n",
    "            ## Blink counter logic\n",
    "            if ratio_eyes > 5.0:\n",
    "                cv.putText(frame, 'Blink', (200,30), FONTS, 1.3, utils.RED, 2)\n",
    "                closed_eyes_counter += 1\n",
    "            else:\n",
    "                if closed_eyes_counter > closed_eyes_frame:\n",
    "                    total_blinks += 1\n",
    "                    closed_eyes_counter = 0\n",
    "            cv.putText(frame, f'Total blinks {total_blinks}', (100,150), FONTS, 0.6, utils.GREEN, 1)\n",
    "\n",
    "\n",
    "            ## Yawn\n",
    "            ratio_mouth = yawnRatio(frame, mesh_coords, LIPS)\n",
    "            #cv.putText(frame, f'ratio {round(ratio_mouth,2)}', (100,100), FONTS, 1.0, utils.GREEN, 1)\n",
    "       \n",
    "            \n",
    "            \n",
    "            ## calculating  frame per seconds FPS\n",
    "            end_time = time.time()-start_time\n",
    "            fps = frame_counter/end_time\n",
    "            frame =utils.textWithBackground(frame,f'FPS: {round(fps,1)}',FONTS, 1.0, (20, 50), bgOpacity=0.9, textThickness=2)\n",
    "\n",
    "\n",
    "        cv.imshow('Webcam', frame)\n",
    "\n",
    "        # check if any keys are being pressed, if this is 'q' break out of while loop and release webcam & close frame\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('computer_vision')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0777b629fee9d2d81ecef945347a78c44178a54bd3f588c64ea08ff012c2b82a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
